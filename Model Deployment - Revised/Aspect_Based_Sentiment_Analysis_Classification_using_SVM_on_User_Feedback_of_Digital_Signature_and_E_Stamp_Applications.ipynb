{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLapWLDzw4qZtJIvfQzmj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/efrizac/ABSA-using-SVM-on-User-Feedback-of-Digital-Signature-and-E-Stamp-Applications/blob/main/Aspect_Based_Sentiment_Analysis_Classification_using_SVM_on_User_Feedback_of_Digital_Signature_and_E_Stamp_Applications.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Scraping"
      ],
      "metadata": {
        "id": "puUntzOCA0tv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json, os, uuid"
      ],
      "metadata": {
        "id": "XnxCuZv3BDwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZASF5GOf8EzZ"
      },
      "outputs": [],
      "source": [
        "!pip install app_store_scraper\n",
        "from app_store_scraper import AppStore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper\n",
        "from google_play_scraper import app"
      ],
      "metadata": {
        "id": "SYQH5oc0A913"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scraping Google Play Store"
      ],
      "metadata": {
        "id": "GcsIYlFGBFji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### All Review"
      ],
      "metadata": {
        "id": "okfUiXPtBLsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper\n",
        "from google_play_scraper import app\n",
        "\n",
        "from google_play_scraper import Sort, reviews_all\n",
        "\n",
        "g_reviews = reviews_all(\n",
        "    #'id.vida.app',\n",
        "    #'id.co.xignature',\n",
        "    'com.privygate.privyid',\n",
        "    sleep_milliseconds=0,\n",
        "    lang='id',\n",
        "    country='id',\n",
        "    sort=Sort.MOST_RELEVANT,\n",
        ")"
      ],
      "metadata": {
        "id": "5q_mY-DlBI15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Certain Amount"
      ],
      "metadata": {
        "id": "NVAcKttiBaKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google_play_scraper import Sort, reviews\n",
        "\n",
        "g_reviews, continuation_token = reviews(\n",
        "    # 'id.vida.app',\n",
        "    #'id.co.xignature',\n",
        "    'com.privygate.privyid',\n",
        "    lang='id',\n",
        "    country='id',\n",
        "    sort=Sort.NEWEST,\n",
        "    count=700,\n",
        "    filter_score_with=None"
      ],
      "metadata": {
        "id": "O9d5X9s4Bf97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save Data"
      ],
      "metadata": {
        "id": "VLI8WIabCCNZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g_df = pd.DataFrame(np.array(g_reviews),columns=['review'])\n",
        "g_df = g_df.join(pd.DataFrame(g_df.pop('review').tolist()))\n",
        "g_df.head(20)"
      ],
      "metadata": {
        "id": "YBGLW3VsBvNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(g_df.index)"
      ],
      "metadata": {
        "id": "UX7J6-3_CMIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_df[['score','at', 'content']].head(20)"
      ],
      "metadata": {
        "id": "ai7RypzECNXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_df = g_df[['score','at', 'content']]\n",
        "g_df.rename(columns={'score':'rating','content':'review'}, inplace=True)\n",
        "g_df.insert(loc=0, column='source', value='Google Play Store')\n",
        "g_df.insert(loc=1, column='app', value='Privy')\n",
        "g_df = g_df.sort_values(by='at', ascending=False) #scraping sudah dilakukan berdasarkan data terbaru\n",
        "g_df.head(20)"
      ],
      "metadata": {
        "id": "GiCWZ5F0CQr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_df.to_csv('vida_newest_googleplay_reviews.csv', index = False)"
      ],
      "metadata": {
        "id": "x8NGNAD2CU0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_df.to_csv('xignature_newest_googleplay_reviews.csv', index = False)"
      ],
      "metadata": {
        "id": "ZJs97M9VCW0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_df.to_csv('privy_relevant_googleplay_reviews.csv', index = False)"
      ],
      "metadata": {
        "id": "xOep1gKRCYL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "q7lGWpbcCG-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Scraping App Store"
      ],
      "metadata": {
        "id": "wJVQqIMwBnnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install app_store_scraper\n",
        "from app_store_scraper import AppStore\n",
        "\n",
        "#a_reviews = AppStore(\"id\", \"privy\", \"1218828609\")\n",
        "a_reviews = AppStore(\"id\", \"vida\", \"1661262113\")\n",
        "\n",
        "\n",
        "def scrape_reviews(total_reviews, batch_size=400):\n",
        "    current_page = 1\n",
        "    reviews = []\n",
        "\n",
        "    while len(reviews) < total_reviews:\n",
        "        a_reviews.review(how_many=batch_size, after=reviews[-1]['date'] if reviews else None)\n",
        "        new_reviews = a_reviews.reviews\n",
        "\n",
        "        if not new_reviews:\n",
        "            break\n",
        "\n",
        "        reviews.extend(new_reviews)\n",
        "        print(f\"Page {current_page} - Total Reviews Scraped: {len(reviews)}\")\n",
        "\n",
        "        current_page += 1\n",
        "\n",
        "    return reviews\n",
        "\n",
        "reviews_data = scrape_reviews(total_reviews=400, batch_size=400)"
      ],
      "metadata": {
        "id": "2ogsoHiWBqSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save Data"
      ],
      "metadata": {
        "id": "8egSDCLpCwtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_df = pd.DataFrame(np.array(reviews_data), columns=['review'])\n",
        "a_df = a_df.join(pd.DataFrame(a_df.pop('review').tolist()))\n",
        "a_df.head(5)"
      ],
      "metadata": {
        "id": "bNOxBA4PCyH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(a_df.index)"
      ],
      "metadata": {
        "id": "PpQfizPZCznM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_df[['rating','date', 'review']].head(20)"
      ],
      "metadata": {
        "id": "6LxfH4MOC04_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_df = a_df[['rating','date', 'review']]\n",
        "a_df.rename(columns={'date':'at'}, inplace=True)\n",
        "a_df.insert(loc=0, column='source', value='App Store')\n",
        "a_df.insert(loc=1, column='app', value='Vida')\n",
        "sorted_a_df = a_df.sort_values(by='at', ascending=False) #scraping sudah dilakukan berdasarkan data terbaru\n",
        "sorted_a_df.head(20)"
      ],
      "metadata": {
        "id": "mzaFF1yeC2PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_a_df.to_csv('vida_appstore_reviews.csv', index = False)"
      ],
      "metadata": {
        "id": "GNsLXd6sC38D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_a_df.to_csv('privy_appstore_reviews.csv', index = False)"
      ],
      "metadata": {
        "id": "mk_JrDz8C6Fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Merge Data"
      ],
      "metadata": {
        "id": "qUzT7EZ3C7Dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UzMgXdbgDCpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "privy_g_df = pd.read_csv('/content/drive/MyDrive/privy_newest_googleplay_reviews.csv')\n",
        "vida_g_df = pd.read_csv('/content/drive/MyDrive/vida_newest_googleplay_reviews.csv')\n",
        "xignature_g_df = pd.read_csv('/content/drive/MyDrive/xignature_newest_googleplay_reviews.csv')\n",
        "vida_a_df = pd.read_csv('/content/drive/MyDrive/vida_appstore_reviews.csv')\n",
        "privy_a_df = pd.read_csv('/content/drive/MyDrive/privy_appstore_reviews.csv')"
      ],
      "metadata": {
        "id": "0Bh3xGhPDADE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([privy_g_df, vida_g_df, xignature_g_df, vida_a_df, privy_a_df], ignore_index=True)\n",
        "sorted_merged_df = merged_df.sort_values(by='at', ascending=False)\n",
        "sorted_merged_df.to_csv('merged_reviews.csv', index = False)\n",
        "sorted_merged_df.shape"
      ],
      "metadata": {
        "id": "Xh3q05flDH97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Exploration"
      ],
      "metadata": {
        "id": "MbbiNsvTDJYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Data_Skripsi/merged_reviews.csv')"
      ],
      "metadata": {
        "id": "0octIBmgDKw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "XiB7JGlxDUnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.head(5))\n",
        "print('----------')\n",
        "print('Shape: ' , data.shape)\n",
        "print('----------')\n",
        "print(data.dtypes)\n",
        "print('----------')\n",
        "print(data.describe())\n",
        "print('----------')\n",
        "print('Data Null?')\n",
        "print(data.isnull().sum())"
      ],
      "metadata": {
        "id": "TN3NKD43DU9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "review_counts = data.groupby(['app', 'source'])['review'].count().reset_index()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='app', y='review', hue='source', data=review_counts)\n",
        "plt.title('Jumlah Ulasan per Sumber and Aplikasi')\n",
        "plt.xlabel('Aplikasi')\n",
        "plt.ylabel('Jumlah Ulasan')\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{int(p.get_height())}',\n",
        "                (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center',\n",
        "                fontsize=10, color='black',\n",
        "                xytext=(0, 5), textcoords='offset points')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FcUySSE9DgAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text = \" \".join(review for review in data.review)\n",
        "print (\"There are {} words in the combination of all review.\".format(len(text)))\n",
        "wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
        "\n",
        "plt.figure(figsize=(10, 5), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SRr9Y6i_DWLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing"
      ],
      "metadata": {
        "id": "DbYYkHTsDoEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "LfVqdIKEEBdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Eliminate Data Duplication"
      ],
      "metadata": {
        "id": "x7a8SoiQDtVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_duplicate_rows(data) :\n",
        "  duplicate_rows = data[data.duplicated(keep=False)]\n",
        "  return duplicate_rows\n",
        "\n",
        "duplicate_rows = check_duplicate_rows(data)\n",
        "\n",
        "if not duplicate_rows.empty:\n",
        "  print(\"Baris duplikat ditemukan:\")\n",
        "  print(duplicate_rows)\n",
        "else:\n",
        "  print(\"Tidak ditemukan baris duplikat.\")"
      ],
      "metadata": {
        "id": "cpYvvnfLDsUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_duplication(data):\n",
        "  data.drop_duplicates(subset=['review'], keep='first', inplace=True)\n",
        "  return data\n",
        "\n",
        "data = remove_duplication(data)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "bya0-x4_D0WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Eliminate Short Reviews"
      ],
      "metadata": {
        "id": "BxYK0ifID1JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_words(data):\n",
        "  short_reviews = data[data['review'].str.split().str.len() < 3]\n",
        "  return short_reviews\n",
        "\n",
        "short_reviews = check_words(data)\n",
        "\n",
        "if not short_reviews.empty:\n",
        "  print(\"Baris dengan review kurang dari 3 kata:\")\n",
        "  print(short_reviews)\n",
        "else:\n",
        "  print(\"Tidak ditemukan baris dengan review kurang dari 3 kata.\")"
      ],
      "metadata": {
        "id": "zqWA-PmOD3gF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_reviews(data):\n",
        "  data = data[data['review'].str.split().str.len() >= 3]\n",
        "  return data\n",
        "\n",
        "data = remove_short_reviews(data)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "AjZOd-jAD5-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Cleaning & Case Folding"
      ],
      "metadata": {
        "id": "w0RmHCVsEDYr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def case_folding(text):\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "pdtyP0-REF_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()  # Mengubah teks menjadi huruf kecil\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Menghapus karakter khusus dan angka\n",
        "  text = re.sub(r'[^\\w\\s]', '', text) # Menghapus karakter non-alphanumeric dan whitespace\n",
        "  text = re.sub(r'\\s+', ' ', text).strip()  # Menghapus whitespace ekstra\n",
        "  return text\n",
        "\n",
        "data['cleaned_review'] = data['review'].apply(clean_text)\n",
        "\n",
        "comparison_df = data[['source', 'app', 'review', 'cleaned_review']]\n",
        "print(comparison_df.head(10))\n",
        "\n",
        "comparison_df.to_csv('review_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "j8IAUvC3EHgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "1Ps8wKxQNwUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  return word_tokenize(text)"
      ],
      "metadata": {
        "id": "zf8i9kMkNyFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "  return word_tokenize(text)\n",
        "\n",
        "data['tokenized_review'] = data['cleaned_review'].apply(tokenize_text)\n",
        "\n",
        "comparison_tokenized_df = data[['source', 'app', 'review', 'cleaned_review', 'tokenized_review']]\n",
        "print(comparison_tokenized_df.head(10))\n",
        "\n",
        "comparison_tokenized_df.to_csv('tokenized_review_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "utD17UHyN1tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalization"
      ],
      "metadata": {
        "id": "8BAylVj_N26v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizad_word = pd.read_csv('/content/drive/MyDrive/Data_Skripsi/kamusalay.csv', header=None)\n",
        "normalizad_word_dict = {row[0]: row[1] for _, row in normalizad_word.iterrows()}\n",
        "\n",
        "def normalize_term(tokens):\n",
        "    return [normalizad_word_dict.get(term, term) for term in tokens]"
      ],
      "metadata": {
        "id": "mOMIJYcHN4c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizad_word = pd.read_csv('/content/drive/MyDrive/Data_Skripsi/kamusalay.csv', header=None)  # Memuat kamus alay (file .csv kamus normalisasi)\n",
        "normalizad_word_dict = {row[0]: row[1] for _, row in normalizad_word.iterrows()}  # Membuat kamus normalisasi dari csv\n",
        "\n",
        "def normalize_term(tokens):\n",
        "    return [normalizad_word_dict.get(term, term) for term in tokens]  # Jika term ada di kamus, normalisasikan, jika tidak tetap\n",
        "\n",
        "data['normalized_review'] = data['tokenized_review'].apply(normalize_term)\n",
        "\n",
        "comparison_normalization_df = data[['source', 'app', 'review', 'tokenized_review', 'normalized_review']]\n",
        "print(comparison_normalization_df.head(10))\n",
        "\n",
        "\n",
        "comparison_normalization_df.to_csv('normalization_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "vMoVPMrXN6Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopword Removal"
      ],
      "metadata": {
        "id": "ISUAwiAXN808"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "\n",
        "stop_words.update([\"aplikasi\", \"privy\", \"privyid\", \"vida\", \"xignature\", \"akulaku\"])\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "  filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "  return filtered_tokens\n",
        "\n",
        "data['filtered_review'] = data['normalized_review'].apply(remove_stopwords)\n",
        "\n",
        "comparison_stopwords_df = data[['source', 'app', 'review', 'normalized_review', 'filtered_review']]\n",
        "print(comparison_stopwords_df.head(10))\n",
        "\n",
        "comparison_stopwords_df.to_csv('stopwords_removal_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "TWQgOW3FOABJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "A3M-rWeZOBjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Membuat stemmer\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemming_text(tokens):\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "  return stemmed_tokens\n",
        "\n",
        "data['stemmed_review'] = data['filtered_review'].apply(stemming_text)\n",
        "\n",
        "comparison_stemming_df = data[['source', 'app', 'review', 'filtered_review', 'stemmed_review']]\n",
        "print(comparison_stemming_df.head(10))\n",
        "\n",
        "comparison_stemming_df.to_csv('stemming_comparison.csv', index=False)"
      ],
      "metadata": {
        "id": "04M2wYdPODuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Topic Modeling Using LDA (Latent Dirichlet Allocation)"
      ],
      "metadata": {
        "id": "AQZHZQTUOEia"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "4YHCeW8xOHxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.tokenize import word_tokenize\n",
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "qMNFYK2xOOSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/preprocessed_data.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "jfb3CDNaOT7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dictionary = corpora.Dictionary(data['stemmed_review'])\n",
        "corpus = [dictionary.doc2bow(text) for text in data['stemmed_review']]\n",
        "\n",
        "coherence_values = []\n",
        "perplexity_values = []\n",
        "num_topics_range = range(2, 12)\n",
        "\n",
        "for num_topics in num_topics_range:\n",
        "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=42, passes=10, alpha='auto')\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=data['stemmed_review'], dictionary=dictionary, coherence='c_v')\n",
        "    coherence_values.append(coherence_model.get_coherence())\n",
        "    perplexity_values.append(lda_model.log_perplexity(corpus))\n",
        "\n",
        "coherence_df = pd.DataFrame({'Jumlah Topik': num_topics_range, 'Nilai Koherensi': coherence_values})"
      ],
      "metadata": {
        "id": "51E6_MklOa2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat model LDA\n",
        "dictionary = corpora.Dictionary(data['stemmed_review'])\n",
        "corpus = [dictionary.doc2bow(text) for text in data['stemmed_review']]\n",
        "optimal_num_topics = 4 # Sesuaikan dengan jumlah topik yang optimal\n",
        "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=optimal_num_topics, random_state=42, passes=10, alpha='auto')\n",
        "\n",
        "# Menampilkan topik yang ditemukan\n",
        "print(\"Topik yang ditemukan:\")\n",
        "topics = lda_model.print_topics(num_words=15)\n",
        "for idx, topic in topics:\n",
        "    print(f\"Topik {idx}:\\n{topic}\\n\")\n",
        "\n",
        "print(\"Keyword per Topik\")\n",
        "topics = lda_model.print_topics(num_words=60)\n",
        "for idx, topic in topics:\n",
        "    print(f\"Topik {idx}:\\n{topic}\\n\")"
      ],
      "metadata": {
        "id": "ebikiI3qOgM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis\n",
        "\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "import pyLDAvis\n",
        "\n",
        "# Visualisasi LDA dengan pyLDAvis\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
        "pyLDAvis.display(vis)"
      ],
      "metadata": {
        "id": "BWpSyXE6O3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Labeling Data"
      ],
      "metadata": {
        "id": "gsBsq04jO6Jp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "xtoc-JIzO7Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Aspect Labeling"
      ],
      "metadata": {
        "id": "ZU6GueBUQn6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic0 = [\n",
        "   \"daftar\", \"email\", \"masuk\", \"susah\", \"verifikasi\", \"login\", \"id\", \"tolong\", \"ya\", \"nik\", \"kali\", \"gagal\", \"akun\", \"ganti\", \"foto\", \"sih\", \"password\", \"ktp\", \"coba\", \"mohon\", \"data\", \"ulang\", \"swafoto\", \"menit\", \"liveness\", \"kamera\", \"register\", \"reg\", \"vermuk\", \"face\"\n",
        "]\n",
        "\n",
        "topic1 = [\n",
        "    \"mudah\", \"bantu\", \"tanda\", \"tangan\", \"banget\", \"bagus\", \"digital\", \"ribet\", \"ttd\", \"dokumen\", \"kerja\", \"bikin\", \"aplikasi\", \"guna\", \"app\", \"keren\", \"aplikasi\", \"mantap\", \"recommend\"\n",
        "]\n",
        "\n",
        "topic2 = [\n",
        "    \"respon\", \"cepat\", \"layan\", \"cs\", \"chat\", \"kasih\", \"kirim\", \"dm\", \"instagram\", \"terima\", \"tanggap\", \"balas\", \"live\", \"fast\", \"ramah\", \"solusi\", \"terimakasih\", \"live\", \"helpdesk\", \"admin\", \"service\", \"komplain\"\n",
        "]\n",
        "\n",
        "topic3 = [\n",
        "    \"buka\", \"update\", \"pakai\", \"koneksi\", \"download\", \"jaring\", \"apk\", \"youtube\", \"eror\", \"error\", \"internet\", \"bug\", \"lot\", \"lelet\", \"baik\", \"instal\", \"install\", \"hitam\", \"lemot\", \"loading\", \"server\", \"lag\", \"upgrade\", \"store\", \"maintenance\"\n",
        "]\n",
        "\n",
        "def label_aspects(stemmed_review):\n",
        "    topic0_label = 1 if any(keyword in stemmed_review for keyword in topic0) else 0\n",
        "    topic1_label = 1 if any(keyword in stemmed_review for keyword in topic1) else 0\n",
        "    topic2_label = 1 if any(keyword in stemmed_review for keyword in topic2) else 0\n",
        "    topic3_label = 1 if any(keyword in stemmed_review for keyword in topic3) else 0\n",
        "\n",
        "    return pd.Series({\n",
        "        'Login dan Verifikasi': topic0_label,\n",
        "        'Efisiensi': topic1_label,\n",
        "        'Layanan Pengguna': topic2_label,\n",
        "        'Performa': topic3_label\n",
        "    })\n",
        "\n",
        "data[['Login dan Verifikasi', 'Efisiensi', 'Layanan Pengguna', 'Performa']] = data['stemmed_review'].apply(label_aspects)\n",
        "data['total_aspek'] = data[['Login dan Verifikasi', 'Efisiensi', 'Layanan Pengguna', 'Performa']].sum(axis=1)\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "tqW58-TOPKPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Sentiment Labeling"
      ],
      "metadata": {
        "id": "y_gHHza-QqIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mengubah angka 1 menjadi \"Relevant\"\n",
        "columns_to_replace = ['Login dan Verifikasi (T)', 'Efisiensi (T)', 'Layanan Pengguna (T)', 'Responsivitas/Performa (T)']\n",
        "for col in columns_to_replace:\n",
        "    data.loc[pd.notnull(data[col]), col] = \"Relevant\"\n",
        "\n",
        "data.to_csv('annotator_data.csv', index=False)\n",
        "data"
      ],
      "metadata": {
        "id": "l1f9lV19Q1Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "ann1 = pd.read_csv('/content/drive/MyDrive/annotator_data_sentiment_1.csv')\n",
        "ann2 = pd.read_csv('/content/drive/MyDrive/annotator_data_sentiment_2.csv')\n",
        "ann3 = pd.read_csv('/content/drive/MyDrive/annotator_data_sentiment_3.csv')\n",
        "\n",
        "merged_df = pd.DataFrame({\n",
        "  'review' : ann1['review'],\n",
        "  'stemmed_review' : ann1['stemmed_review'],\n",
        "  'login_verifikasi_1' : ann1['Login dan Verifikasi (T)'],\n",
        "  'login_verifikasi_2' : ann2['Login dan Verifikasi (T)'],\n",
        "  'login_verifikasi_3' : ann3['Login dan Verifikasi (T)'],\n",
        "  'efisiensi_1' : ann1['Efisiensi (T)'],\n",
        "  'efisiensi_2' : ann2['Efisiensi (T)'],\n",
        "  'efisiensi_3' : ann3['Efisiensi (T)'],\n",
        "  'layanan_pengguna_1' : ann1['Layanan Pengguna (T)'],\n",
        "  'layanan_pengguna_2' : ann2['Layanan Pengguna (T)'],\n",
        "  'layanan_pengguna_3' : ann3['Layanan Pengguna (T)'],\n",
        "  'responsivitas_1' : ann1['Responsivitas/Performa (T)'],\n",
        "  'responsivitas_2' : ann2['Responsivitas/Performa (T)'],\n",
        "  'responsivitas_3' : ann3['Responsivitas/Performa (T)'],\n",
        "})"
      ],
      "metadata": {
        "id": "nwZju2RmRTjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "merged_df = merged_df.fillna('0')\n",
        "merged_df = merged_df.replace('IR', '0')\n",
        "\n",
        "# Membersihkan nilai dan menghilangkan desimal\n",
        "def clean_value(x):\n",
        "    if pd.isna(x):  # Jika nilai NaN, biarkan sebagai NaN\n",
        "        return x\n",
        "    try:\n",
        "        x = float(x)  # Ubah ke float terlebih dahulu\n",
        "        return int(x)  # Ubah ke integer untuk menghilangkan titik desimal\n",
        "    except (ValueError, TypeError):\n",
        "        return x\n",
        "\n",
        "merged_df = merged_df.applymap(clean_value)\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "s8KR69cLRdac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_sentiment(row, columns):\n",
        "    # Hitung frekuensi setiap nilai di kolom yang diberikan\n",
        "    sentiment_counts = Counter([row[col] for col in columns])\n",
        "    most_common_sentiment = sentiment_counts.most_common(1)[0]\n",
        "\n",
        "    # Jika mayoritas sentimen muncul dua kali atau lebih\n",
        "    if most_common_sentiment[1] >= 2:\n",
        "        return most_common_sentiment[0]\n",
        "    return 'equal'\n",
        "\n",
        "# Ambil nilai dominan untuk setiap aspek\n",
        "merged_df['final_login_verifikasi'] = merged_df.apply(\n",
        "    lambda row: get_final_sentiment(row, ['login_verifikasi_1', 'login_verifikasi_2', 'login_verifikasi_3']), axis=1)\n",
        "\n",
        "merged_df['final_efisiensi'] = merged_df.apply(\n",
        "    lambda row: get_final_sentiment(row, ['efisiensi_1', 'efisiensi_2', 'efisiensi_3']), axis=1)\n",
        "\n",
        "merged_df['final_layanan_pengguna'] = merged_df.apply(\n",
        "    lambda row: get_final_sentiment(row, ['layanan_pengguna_1', 'layanan_pengguna_2', 'layanan_pengguna_3']), axis=1)\n",
        "\n",
        "merged_df['final_responsivitas'] = merged_df.apply(\n",
        "    lambda row: get_final_sentiment(row, ['responsivitas_1', 'responsivitas_2', 'responsivitas_3']), axis=1)\n",
        "\n",
        "merged_df.insert(5, 'final_login_verifikasi', merged_df.pop('final_login_verifikasi'))\n",
        "merged_df.insert(9, 'final_efisiensi', merged_df.pop('final_efisiensi'))\n",
        "merged_df.insert(13, 'final_layanan_pengguna', merged_df.pop('final_layanan_pengguna'))\n",
        "merged_df.insert(17, 'final_responsivitas', merged_df.pop('final_responsivitas'))\n",
        "\n",
        "merged_df\n",
        "merged_df.to_csv(\"final_sentiment_labeling_results.csv\", index=False)"
      ],
      "metadata": {
        "id": "U6lwkFM3RpCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Labeling Result Evaluation"
      ],
      "metadata": {
        "id": "wngR_A9VRxDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Krippendorff Alpha Calculation"
      ],
      "metadata": {
        "id": "mFuByLs-SL70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install krippendorff\n",
        "import krippendorff\n",
        "\n",
        "def prepare_data_for_alpha(data, annotator_columns):\n",
        "    data_subset = data[annotator_columns]\n",
        "    data_alpha = data_subset.transpose().values.tolist()\n",
        "    return data_alpha\n",
        "\n",
        "login_verifikasi_data = prepare_data_for_alpha(\n",
        "    merged_df,\n",
        "    ['login_verifikasi_1', 'login_verifikasi_2', 'login_verifikasi_3']\n",
        ")\n",
        "efisiensi_data = prepare_data_for_alpha(\n",
        "    merged_df,\n",
        "    ['efisiensi_1', 'efisiensi_2', 'efisiensi_3']\n",
        ")\n",
        "layanan_pengguna_data = prepare_data_for_alpha(\n",
        "    merged_df,\n",
        "    ['layanan_pengguna_1', 'layanan_pengguna_2', 'layanan_pengguna_3']\n",
        ")\n",
        "responsivitas_data = prepare_data_for_alpha(\n",
        "    merged_df,\n",
        "    ['responsivitas_1', 'responsivitas_2', 'responsivitas_3']\n",
        ")\n",
        "\n",
        "alpha_login_verifikasi = krippendorff.alpha(reliability_data=login_verifikasi_data, level_of_measurement='nominal')\n",
        "alpha_efisiensi = krippendorff.alpha(reliability_data=efisiensi_data, level_of_measurement='nominal')\n",
        "alpha_layanan_pengguna = krippendorff.alpha(reliability_data=layanan_pengguna_data, level_of_measurement='nominal')\n",
        "alpha_responsivitas = krippendorff.alpha(reliability_data=responsivitas_data, level_of_measurement='nominal')"
      ],
      "metadata": {
        "id": "t7Tpnki8R1BR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Imbalance Ratio Calculation"
      ],
      "metadata": {
        "id": "vVfmwPBFSTi6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data_Skripsi/final_sentiment_labeling_results.csv')\n",
        "data = data[['final_login_verifikasi', 'final_efisiensi', 'final_layanan_pengguna', 'final_responsivitas']]\n",
        "\n",
        "def calculate_imbalance_metrics(y_data):\n",
        "    irlbl = []\n",
        "\n",
        "    for column in y_data.columns:\n",
        "        counts = y_data[column].value_counts()\n",
        "\n",
        "        count_negative = counts.get(1, 0)\n",
        "        count_positive = counts.get(2, 0)\n",
        "\n",
        "        min_count = min(count_negative, count_positive)\n",
        "        max_count = max(count_negative, count_positive)\n",
        "        irlbl_value = max_count / (min_count + 1e-6) if min_count > 0 else np.inf\n",
        "        irlbl.append(irlbl_value)\n",
        "\n",
        "    irlbl = np.array(irlbl)\n",
        "\n",
        "    # MeanIR, MaxIR, CVIR\n",
        "    mean_ir = np.mean(irlbl)\n",
        "    max_ir = np.max(irlbl)\n",
        "    cvir = np.std(irlbl) / mean_ir if mean_ir > 0 else np.inf\n",
        "\n",
        "    return {\n",
        "        \"IRLbl\": irlbl,\n",
        "        \"MeanIR\": mean_ir,\n",
        "        \"MaxIR\": max_ir,\n",
        "        \"CVIR\": cvir\n",
        "    }\n",
        "\n",
        "metrics = calculate_imbalance_metrics(data)\n",
        "\n",
        "print(\"Imbalance Metrics:\")\n",
        "print(\"IRLbl per label:\", metrics[\"IRLbl\"])\n",
        "print(\"MeanIR:\", metrics[\"MeanIR\"])\n",
        "print(\"MaxIR:\", metrics[\"MaxIR\"])\n",
        "print(\"CVIR:\", metrics[\"CVIR\"])\n"
      ],
      "metadata": {
        "id": "JyMC7pHfSWh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization"
      ],
      "metadata": {
        "id": "_6n0AWWUSyb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in columns:\n",
        "    data[column] = data[column].astype(str).replace({\n",
        "        '1': \"negative\",\n",
        "        '1.0': \"negative\",\n",
        "        '2': \"positive\",\n",
        "        '2.0': \"positive\"\n",
        "    })\n",
        "\n",
        "def clean_column(col):\n",
        "    return (col.astype(str)\n",
        "              .str.strip()\n",
        "              .replace({'1': \"negative\", '2': \"positive\",\n",
        "                       '1.0': \"negative\", '2.0': \"positive\"}))\n",
        "\n",
        "for column in columns:\n",
        "    data[column] = clean_column(data[column])\n",
        "\n",
        "for column in columns:\n",
        "    print(f\"\\nDistribusi nilai di {column}:\")\n",
        "    print(data[column].value_counts(dropna=False))"
      ],
      "metadata": {
        "id": "h7bkvRJqTJGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_reviews = data[\n",
        "    (data['final_login_verifikasi'] == 'positive') |\n",
        "    (data['final_efisiensi'] == 'positive') |\n",
        "    (data['final_layanan_pengguna'] == 'positive') |\n",
        "    (data['final_responsivitas'] == 'positive')\n",
        "]['review']\n",
        "\n",
        "negative_reviews = data[\n",
        "    (data['final_login_verifikasi'] == 'negative') |\n",
        "    (data['final_efisiensi'] == 'negative') |\n",
        "    (data['final_layanan_pengguna'] == 'negative') |\n",
        "    (data['final_responsivitas'] == 'negative')\n",
        "]['review']\n",
        "\n",
        "positive_text = \" \".join(review for review in positive_reviews.dropna())\n",
        "negative_text = \" \".join(review for review in negative_reviews.dropna())\n",
        "\n",
        "if positive_text:\n",
        "    wordcloud_positive = WordCloud(width=800, height=400, background_color=\"white\").generate(positive_text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title('Positive Sentiment WordCloud')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Tidak ada ulasan positif untuk membuat WordCloud.\")\n",
        "\n",
        "if negative_text:\n",
        "    wordcloud_negative = WordCloud(width=800, height=400, background_color=\"white\").generate(negative_text)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.title('Negative Sentiment WordCloud')\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Tidak ada ulasan negatif untuk membuat WordCloud.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "1-jevguvSzpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aspect_data_filtered = data.replace('irrelevant', np.nan).replace('equal', np.nan).dropna(how='all', subset=aspect_columns)\n",
        "\n",
        "sentiment_comparison = {}\n",
        "for col, label in zip(aspect_columns, aspect_labels):\n",
        "    counts = aspect_data_filtered[col].value_counts()\n",
        "    sentiment_comparison[label] = {\n",
        "        'Positive': counts.get('positive', 0),\n",
        "        'Negative': counts.get('negative', 0),\n",
        "        'Equal': data[col].value_counts().get('equal', 0)\n",
        "    }\n",
        "\n",
        "sentiment_df = pd.DataFrame(sentiment_comparison).T\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "sentiment_df[['Positive', 'Negative', 'Equal']].plot(kind='bar', ax=ax, color=['green', 'red', 'gray'])\n",
        "\n",
        "plt.title('Comparison of Positive, Negative and Equal Sentiments per Aspect')\n",
        "plt.xlabel('Aspects')\n",
        "plt.ylabel('Number of Reviews')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Sentiments')\n",
        "\n",
        "for container in ax.containers:\n",
        "    for bar in container:\n",
        "        height = bar.get_height()\n",
        "        if height > 0:\n",
        "            ax.annotate(f'{int(height)}',\n",
        "                        xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                        xytext=(0, 5),  # Offset\n",
        "                        textcoords='offset points',\n",
        "                        ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Lr35ZTcRS-h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Modeling"
      ],
      "metadata": {
        "id": "G42oMTY1Thr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Data_Skripsi/data_split.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "JPZYLhMPT4fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitting Data"
      ],
      "metadata": {
        "id": "SR6vr9llTnRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn\n",
        "import skmultilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split"
      ],
      "metadata": {
        "id": "XlbyHWq1Tl09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = data[['review', 'stemmed_review']].values  # Fitur\n",
        "y = data[['final_login_verifikasi', 'final_efisiensi',\n",
        "          'final_layanan_pengguna', 'final_responsivitas']].values  # Target multilabel\n",
        "\n",
        "x_model, y_model, x_valid, y_valid = iterative_train_test_split(\n",
        "    x, y, test_size=0.1\n",
        ")\n",
        "\n",
        "print(\"Jumlah data pada train set (x_model):\", x_model.shape)\n",
        "print(\"Jumlah data pada train label (y_model):\", y_model.shape)\n",
        "print(\"Jumlah data pada test set (x_valid):\", x_valid.shape)\n",
        "print(\"Jumlah data pada test label (y_valid):\", y_valid.shape)"
      ],
      "metadata": {
        "id": "_8i6jiXbUIZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save Validation Data"
      ],
      "metadata": {
        "id": "Yhak5R98UP9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_valid = pd.DataFrame(x_valid, columns=['review', 'stemmed_review'])\n",
        "labels_valid = pd.DataFrame(y_valid, columns=['final_login_verifikasi', 'final_efisiensi',\n",
        "                                              'final_layanan_pengguna', 'final_responsivitas'])\n",
        "data_valid = pd.concat([data_valid, labels_valid], axis=1)\n",
        "\n",
        "data_valid.to_csv('data_validasi.csv', index=False)\n",
        "\n",
        "print(\"Data validasi berhasil disimpan ke 'data_validasi.csv'\")"
      ],
      "metadata": {
        "id": "gYlPFB32UMhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Save Modeling Data"
      ],
      "metadata": {
        "id": "rUGK2VOCUSkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_model = pd.DataFrame(x_model, columns=['review', 'stemmed_review'])\n",
        "labels_model = pd.DataFrame(y_model, columns=['final_login_verifikasi', 'final_efisiensi',\n",
        "                                              'final_layanan_pengguna', 'final_responsivitas'])\n",
        "data_model = pd.concat([data_model, labels_model], axis=1)\n",
        "\n",
        "data_model.to_csv('data_model.csv', index=False)\n",
        "\n",
        "print(\"Data model berhasil disimpan ke 'data_model.csv'\")"
      ],
      "metadata": {
        "id": "ynLS0zL_UT13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Modeling"
      ],
      "metadata": {
        "id": "NF23mRRPUbQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Modeling Data Split"
      ],
      "metadata": {
        "id": "vYW8pABRUd1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-multilearn\n",
        "import skmultilearn\n",
        "from skmultilearn.model_selection import iterative_train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "p65iCB21UdGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_80, y_train_80, x_test_20, y_test_20 = iterative_train_test_split(x_model, y_model, test_size=0.2)\n",
        "\n",
        "print(\"Jumlah data pada train set setelah splitting kedua (x_train_80):\", x_train_80.shape)\n",
        "print(\"Jumlah data pada train label setelah splitting kedua (y_train_80):\", y_train_80.shape)\n",
        "print(\"Jumlah data pada test set setelah splitting kedua (x_test_20):\", x_test_20.shape)\n",
        "print(\"Jumlah data pada test label setelah splitting kedua (y_test_20):\", y_test_20.shape)"
      ],
      "metadata": {
        "id": "-wTt8seeUjbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_70, y_train_70, x_test_30, y_test_30 = iterative_train_test_split(x_model, y_model, test_size=0.3)\n",
        "\n",
        "print(\"Jumlah data pada train set setelah splitting kedua (x_train_70):\", x_train_70.shape)\n",
        "print(\"Jumlah data pada train label setelah splitting kedua (y_train_70):\", y_train_70.shape)\n",
        "print(\"Jumlah data pada test set setelah splitting kedua (x_test_30):\", x_test_30.shape)\n",
        "print(\"Jumlah data pada test label setelah splitting kedua (y_test_30):\", y_test_30.shape)"
      ],
      "metadata": {
        "id": "fH5MTR6jUmDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###TF-IDF"
      ],
      "metadata": {
        "id": "N2oIeFtxUrE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "x_train_80 = pd.DataFrame(x_train_80, columns=['review', 'stemmed_review'])\n",
        "x_test_20 = pd.DataFrame(x_test_20, columns=['review', 'stemmed_review'])\n",
        "x_train_70 = pd.DataFrame(x_train_70, columns=['review', 'stemmed_review'])\n",
        "x_test_30 = pd.DataFrame(x_test_30, columns=['review', 'stemmed_review'])\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# TF-IDF untuk split 80:20\n",
        "tfidf_vectorizer_80 = TfidfVectorizer()\n",
        "x_train_tfidf_80 = tfidf_vectorizer_80.fit_transform(x_train_80['stemmed_review'])\n",
        "x_test_tfidf_20 = tfidf_vectorizer_80.transform(x_test_20['stemmed_review'])\n",
        "\n",
        "# TF-IDF untuk split 70:30\n",
        "tfidf_vectorizer_70 = TfidfVectorizer()\n",
        "x_train_tfidf_70 = tfidf_vectorizer_70.fit_transform(x_train_70['stemmed_review'])\n",
        "x_test_tfidf_30 = tfidf_vectorizer_70.transform(x_test_30['stemmed_review'])\n",
        "\n",
        "print(\"Jumlah fitur TF-IDF (80:20):\", len(tfidf_vectorizer_80.get_feature_names_out()))\n",
        "print(\"Contoh fitur TF-IDF (80:20):\", tfidf_vectorizer_80.get_feature_names_out()[:5])\n",
        "\n",
        "print(\"Jumlah fitur TF-IDF (70:30):\", len(tfidf_vectorizer_70.get_feature_names_out()))\n",
        "print(\"Contoh fitur TF-IDF (70:30):\", tfidf_vectorizer_70.get_feature_names_out()[:5])\n",
        "\n",
        "print(\"Shape TF-IDF untuk x_train_tfidf_80:\", x_train_tfidf_80.shape)\n",
        "print(\"Shape TF-IDF untuk x_test_tfidf_20:\", x_test_tfidf_20.shape)\n",
        "print(\"Shape TF-IDF untuk x_train_tfidf_70:\", x_train_tfidf_70.shape)\n",
        "print(\"Shape TF-IDF untuk x_test_tfidf_30:\", x_test_tfidf_30.shape)\n"
      ],
      "metadata": {
        "id": "vAyCqdhaUyrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Resampling"
      ],
      "metadata": {
        "id": "yPGu_TYYVH-o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ML-ROS"
      ],
      "metadata": {
        "id": "5UOeWmPHVRlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import vstack\n",
        "from sklearn.utils import resample\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import resample\n",
        "\n",
        "label_columns = ['final_login_verifikasi', 'final_efisiensi',\n",
        "                 'final_layanan_pengguna', 'final_responsivitas']\n",
        "\n",
        "y_train_80_df = pd.DataFrame(y_train_80, columns=label_columns)\n",
        "y_train_70_df = pd.DataFrame(y_train_70, columns=label_columns)\n",
        "\n",
        "def ml_ros(x_train, y_train):\n",
        "\n",
        "    metrics = calculate_imbalance_metrics(y_train)\n",
        "    irlbl = metrics[\"IRLbl\"]\n",
        "    mean_ir = metrics[\"MeanIR\"]\n",
        "\n",
        "    x_train_resampled = x_train.copy()\n",
        "    y_train_resampled = y_train.copy()\n",
        "\n",
        "    for label_idx, irlbl_value in enumerate(irlbl):\n",
        "        if irlbl_value > mean_ir:\n",
        "            negative_class = y_train.iloc[:, label_idx] == 1\n",
        "            positive_class = y_train.iloc[:, label_idx] == 2\n",
        "\n",
        "            # Jumlah sampel di setiap kelas\n",
        "            count_negative = negative_class.sum()\n",
        "            count_positive = positive_class.sum()\n",
        "\n",
        "            # Kelas minoritas berdasarkan jumlah sampel\n",
        "            if count_negative < count_positive:\n",
        "                minority_indices = np.where(negative_class)[0]\n",
        "            else:\n",
        "                minority_indices = np.where(positive_class)[0]\n",
        "\n",
        "            if len(minority_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            # Jumlah sampel yang perlu ditambahkan\n",
        "            n_samples_to_clone = int(len(minority_indices) * (irlbl_value - 1))\n",
        "            if n_samples_to_clone <= 0: #jika tidak perlu menambahkan label\n",
        "                continue\n",
        "\n",
        "            # Resample fitur dan label\n",
        "            cloned_indices = resample(minority_indices, n_samples=n_samples_to_clone, replace=True, random_state=42) #menambahkan jumlah sampel yang ingin ditambahkan\n",
        "            cloned_features = x_train[cloned_indices, :]\n",
        "            cloned_labels = y_train.iloc[cloned_indices, :]\n",
        "\n",
        "            x_train_resampled = vstack([x_train_resampled, cloned_features])\n",
        "            y_train_resampled = pd.concat([y_train_resampled, cloned_labels], axis=0, ignore_index=True)\n",
        "\n",
        "    # Memastikan ukuran dataset konsisten\n",
        "    assert x_train_resampled.shape[0] == y_train_resampled.shape[0], \"Jumlah sampel antara x_train dan y_train tidak konsisten!\" #validasi ukuran dataset\n",
        "\n",
        "    return x_train_resampled, y_train_resampled\n",
        "\n",
        "# Resampling untuk 80%\n",
        "x_train_mlros_80, y_train_mlros_80 = ml_ros(x_train_tfidf_80, y_train_80_df)\n",
        "print(\"Shape setelah resampling untuk 80%:\", x_train_mlros_80.shape, y_train_mlros_80.shape)\n",
        "\n",
        "# Resampling untuk 70%\n",
        "x_train_mlros_70, y_train_mlros_70 = ml_ros(x_train_tfidf_70, y_train_70_df)\n",
        "print(\"Shape setelah resampling untuk 70%:\", x_train_mlros_70.shape, y_train_mlros_70.shape)\n"
      ],
      "metadata": {
        "id": "_6GRKvaPVKV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ML-SMOTE"
      ],
      "metadata": {
        "id": "bGreFJ8uWQ7D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import vstack\n",
        "import numpy as np\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_columns = ['final_login_verifikasi', 'final_efisiensi',\n",
        "                 'final_layanan_pengguna', 'final_responsivitas']\n",
        "\n",
        "y_train_80_df = pd.DataFrame(y_train_80, columns=label_columns)\n",
        "y_train_70_df = pd.DataFrame(y_train_70, columns=label_columns)\n",
        "\n",
        "def mlsmote(x_train, y_train, k=5):\n",
        "    metrics = calculate_imbalance_metrics(y_train)\n",
        "    irlbl = metrics[\"IRLbl\"]\n",
        "    mean_ir = metrics[\"MeanIR\"]\n",
        "\n",
        "    x_train_resampled = x_train.copy()\n",
        "    y_train_resampled = y_train.copy()\n",
        "\n",
        "    # Konversi sparse matrix menjadi dense\n",
        "    x_train_dense = x_train.toarray() if hasattr(x_train, 'toarray') else x_train\n",
        "\n",
        "    for label_idx, irlbl_value in enumerate(irlbl):\n",
        "        if irlbl_value > mean_ir:\n",
        "            negative_class = y_train.iloc[:, label_idx] == 1\n",
        "            positive_class = y_train.iloc[:, label_idx] == 2\n",
        "\n",
        "            count_negative = negative_class.sum()\n",
        "            count_positive = positive_class.sum()\n",
        "\n",
        "            if count_negative < count_positive:\n",
        "                minority_indices = np.where(negative_class)[0]\n",
        "            else:\n",
        "                minority_indices = np.where(positive_class)[0]\n",
        "\n",
        "            if len(minority_indices) == 0:\n",
        "                continue\n",
        "\n",
        "            minority_samples = x_train_dense[minority_indices]\n",
        "\n",
        "            n_samples_to_generate = int(len(minority_indices) * (irlbl_value - 1))\n",
        "            for _ in range(n_samples_to_generate):\n",
        "                # Pilih sampel minoritas acak\n",
        "                sample_idx = np.random.choice(minority_indices)\n",
        "                sample_features = x_train_dense[sample_idx].reshape(1, -1)\n",
        "\n",
        "                # Hitung jarak dan pilih k tetangga terdekat\n",
        "                distances = pairwise_distances(sample_features, minority_samples)\n",
        "                k_neighbors = min(k, len(minority_samples) - 1)\n",
        "                neighbors_idx = np.argsort(distances.flatten())[:k_neighbors]\n",
        "\n",
        "                random_neighbor_idx = np.random.choice(neighbors_idx)\n",
        "                random_neighbor = minority_samples[random_neighbor_idx].reshape(1, -1)\n",
        "\n",
        "                # Generate synthetic sample\n",
        "                synthetic_sample = sample_features + (random_neighbor - sample_features) * np.random.rand()\n",
        "                synthetic_sample = synthetic_sample.reshape(1, -1)\n",
        "\n",
        "                # Label sintetis: sama dengan label kelas minoritas\n",
        "                synthetic_label = y_train.iloc[sample_idx].copy()\n",
        "\n",
        "                # Tambahkan synthetic sample ke dataset\n",
        "                x_train_resampled = vstack([x_train_resampled, synthetic_sample])\n",
        "                y_train_resampled = pd.concat([y_train_resampled, synthetic_label.to_frame().T], ignore_index=True)\n",
        "\n",
        "    assert x_train_resampled.shape[0] == y_train_resampled.shape[0], \"Jumlah sampel antara x_train dan y_train tidak konsisten!\"\n",
        "\n",
        "# Resampling untuk 80%\n",
        "x_train_mlsmote_80, y_train_mlsmote_80 = mlsmote(x_train_tfidf_80, y_train_80_df)\n",
        "print(\"Shape setelah resampling untuk 80%:\", x_train_mlsmote_80.shape, y_train_mlsmote_80.shape)\n",
        "\n",
        "# Resampling untuk 70%\n",
        "x_train_mlsmote_70, y_train_mlsmote_70 = mlsmote(x_train_tfidf_70, y_train_70_df)\n",
        "print(\"Shape setelah resampling untuk 70%:\", x_train_mlsmote_70.shape, y_train_mlsmote_70.shape)"
      ],
      "metadata": {
        "id": "whxIjrNTWXlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####REMEDIAL"
      ],
      "metadata": {
        "id": "rt8MFGfPW0VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "label_columns = ['final_login_verifikasi', 'final_efisiensi',\n",
        "                 'final_layanan_pengguna', 'final_responsivitas']\n",
        "\n",
        "y_train_80_df = pd.DataFrame(y_train_80, columns=label_columns)\n",
        "y_train_70_df = pd.DataFrame(y_train_70, columns=label_columns)\n",
        "\n",
        "# Menghitung SCUMBLE\n",
        "def calculate_scumble(instance, irlbl):\n",
        "    minority_labels = instance * (irlbl <= np.mean(irlbl))  # Label minoritas\n",
        "    majority_labels = instance * (irlbl > np.mean(irlbl))   # Label mayoritas\n",
        "    scumble = np.sum(minority_labels) / (np.sum(minority_labels) + np.sum(majority_labels))\n",
        "    return scumble if not np.isnan(scumble) else 0\n",
        "\n",
        "# Mengimplementasikan algoritma REMEDIAL\n",
        "def remedial(x_train, y_train):\n",
        "\n",
        "    # Jika x_train adalah sparse matrix, ubah ke DataFrame\n",
        "    if not isinstance(x_train, pd.DataFrame):\n",
        "        x_train = pd.DataFrame(x_train.toarray())\n",
        "\n",
        "    # Hitung IRLbl\n",
        "    imbalance_metrics = calculate_imbalance_metrics(y_train)\n",
        "    irlbl = imbalance_metrics[\"IRLbl\"]\n",
        "\n",
        "    # Hitung rata-rata IRLbl\n",
        "    ir_mean = imbalance_metrics[\"MeanIR\"]\n",
        "\n",
        "    # Hitung SCUMBLE untuk setiap instance\n",
        "    scumble_values = np.array([calculate_scumble(y_train.iloc[i], irlbl) for i in range(len(y_train))])\n",
        "    scumble_threshold = np.mean(scumble_values)\n",
        "\n",
        "    # Proses setiap instance\n",
        "    new_data = []\n",
        "    new_labels = []\n",
        "    for i in range(len(x_train)):\n",
        "        if scumble_values[i] > scumble_threshold:\n",
        "            minority_labels = (y_train.iloc[i] * (irlbl <= ir_mean)).astype(int)\n",
        "            majority_labels = (y_train.iloc[i] * (irlbl > ir_mean)).astype(int)\n",
        "\n",
        "            # Clone instance\n",
        "            new_instance = x_train.iloc[i]\n",
        "\n",
        "            # Maintain minority and majority labels\n",
        "            cloned_labels = minority_labels + majority_labels\n",
        "\n",
        "            # Append cloned instance\n",
        "            new_data.append(new_instance)\n",
        "            new_labels.append(cloned_labels)\n",
        "\n",
        "    # Gabungkan dataset asli dengan data yang baru dibuat\n",
        "    if new_data:\n",
        "        x_train = pd.concat([x_train, pd.DataFrame(new_data, columns=x_train.columns)], ignore_index=True)\n",
        "        y_train = pd.concat([y_train, pd.DataFrame(new_labels, columns=y_train.columns)], ignore_index=True)\n",
        "\n",
        "    return x_train, y_train\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Dataset 80%\n",
        "    x_train_remedial_80, y_train_remedial_80 = remedial(x_train_tfidf_80, y_train_80_df)\n",
        "    print(\"Shape setelah REMEDIAL untuk 80%:\", x_train_remedial_80.shape, y_train_remedial_80.shape)\n",
        "\n",
        "    # Dataset 70%\n",
        "    x_train_remedial_70, y_train_remedial_70 = remedial(x_train_tfidf_70, y_train_70_df)\n",
        "    print(\"Shape setelah REMEDIAL untuk 70%:\", x_train_remedial_70.shape, y_train_remedial_70.shape)\n"
      ],
      "metadata": {
        "id": "a5DgsIctW1f2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Modeling"
      ],
      "metadata": {
        "id": "owMYchGkXTxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, hamming_loss\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import MultiOutputClassifier, ClassifierChain\n",
        "from skmultilearn.problem_transform import BinaryRelevance, LabelPowerset, ClassifierChain"
      ],
      "metadata": {
        "id": "-P5KCU_1XXjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "\n",
        "def compute_hamming(y_true, y_pred):\n",
        "    mlb = MultiLabelBinarizer()\n",
        "\n",
        "    if hasattr(y_pred, \"toarray\"):  # Jika sparse matrix, konversi ke dense\n",
        "        y_pred = y_pred.toarray()\n",
        "    if hasattr(y_true, \"toarray\"):  # Jika sparse matrix, konversi ke dense\n",
        "        y_true = y_true.toarray()\n",
        "\n",
        "    y_true_bin = mlb.fit_transform(y_true)\n",
        "    y_pred_bin = mlb.transform(y_pred)\n",
        "\n",
        "    hamming = hamming_loss(y_true_bin, y_pred_bin)\n",
        "    return hamming, 1 - hamming\n",
        "\n",
        "# Kombinasi skenario\n",
        "splits = ['80', '70']\n",
        "resampling_methods = ['mlros', 'mlsmote', 'remedial']\n",
        "C_values = [0.1, 1, 10]\n",
        "strategies = ['normal', 'cc']\n",
        "\n",
        "# Simpan hasil evaluasi\n",
        "results = []\n",
        "\n",
        "# x_test dan y_test sesuai rasio split\n",
        "x_tests = {\n",
        "    '80': x_test_tfidf_20,\n",
        "    '70': x_test_tfidf_30\n",
        "}\n",
        "y_tests = {\n",
        "    '80': y_test_20,\n",
        "    '70': y_test_30\n",
        "}\n",
        "\n",
        "# Data train yang sudah di-resample sesuai kombinasi\n",
        "x_trains = {\n",
        "    'mlros_80': x_train_mlros_80,\n",
        "    'mlros_70': x_train_mlros_70,\n",
        "    'mlsmote_80': x_train_mlsmote_80,\n",
        "    'mlsmote_70': x_train_mlsmote_70,\n",
        "    'remedial_80': x_train_remedial_80,\n",
        "    'remedial_70': x_train_remedial_70,\n",
        "}\n",
        "y_trains = {\n",
        "    'mlros_80': y_train_mlros_80,\n",
        "    'mlros_70': y_train_mlros_70,\n",
        "    'mlsmote_80': y_train_mlsmote_80,\n",
        "    'mlsmote_70': y_train_mlsmote_70,\n",
        "    'remedial_80': y_train_remedial_80,\n",
        "    'remedial_70': y_train_remedial_70,\n",
        "}\n",
        "\n",
        "# Loop semua kombinasi\n",
        "for split in splits:\n",
        "    for method in resampling_methods:\n",
        "        for C in C_values:\n",
        "            for strategy in strategies:\n",
        "                key = f'{method}_{split}'\n",
        "                x_train = x_trains[key]\n",
        "                y_train = y_trains[key]\n",
        "                x_test = x_tests[split]\n",
        "                y_test = y_tests[split]\n",
        "\n",
        "                model = LinearSVC(C=C, max_iter=10000)\n",
        "\n",
        "                if strategy == 'normal':\n",
        "                    clf = MultiOutputClassifier(model)\n",
        "                elif strategy == 'cc':\n",
        "                    clf = ClassifierChain(model)\n",
        "\n",
        "                clf.fit(x_train, y_train)\n",
        "                y_pred = clf.predict(x_test)\n",
        "\n",
        "                # Hamming Loss\n",
        "                hamming, acc_hamming = compute_hamming(y_test, y_pred)\n",
        "\n",
        "                results.append({\n",
        "                    'Split': split,\n",
        "                    'Resampling': method,\n",
        "                    'C': C,\n",
        "                    'Strategy': strategy,\n",
        "                    'Hamming_Loss': hamming,\n",
        "                    'Hamming_Accuracy': acc_hamming\n",
        "                })\n",
        "\n",
        "result_df = pd.DataFrame(results)\n",
        "display(result_df.sort_values(by='Hamming_Loss'))"
      ],
      "metadata": {
        "id": "Y3PCoP5cYJJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Test set model paling optimal\n",
        "x_test = x_test_tfidf_30\n",
        "y_test = y_test_30\n",
        "\n",
        "# Model dan strategi paling optimal\n",
        "model = LinearSVC(C=1, max_iter=10000)\n",
        "clf = MultiOutputClassifier(model)\n",
        "\n",
        "# Fit model paling optimal\n",
        "clf.fit(x_train_mlros_70, y_train_mlros_70)\n",
        "\n",
        "# Prediksi\n",
        "y_pred = clf.predict(x_test)\n",
        "\n",
        "# Evaluasi per label\n",
        "label_accuracies = []\n",
        "print(\"Model Evaluation\")\n",
        "for i, col in enumerate(['final_login_verifikasi', 'final_efisiensi',\n",
        "                         'final_layanan_pengguna', 'final_responsivitas']):\n",
        "    print(f\"Classification Report for {col}:\")\n",
        "    print(classification_report(y_test[:, i], y_pred[:, i], zero_division=0))\n",
        "    accuracy = accuracy_score(y_test[:, i], y_pred[:, i])\n",
        "    print(f\"Accuracy for {col}: {accuracy}\")\n",
        "    label_accuracies.append(accuracy)\n",
        "    print()\n",
        "\n",
        "# Hitung Macro Average Accuracy\n",
        "macro_avg_accuracy = sum(label_accuracies) / len(label_accuracies)\n",
        "print(\"Macro Average Accuracy:\", macro_avg_accuracy)\n"
      ],
      "metadata": {
        "id": "Ihz6IU2NY0nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model"
      ],
      "metadata": {
        "id": "ceLgLw-MZM5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "\n",
        "# Data yang sesuai\n",
        "x_train = x_trains['mlros_70']\n",
        "y_train = y_trains['mlros_70']\n",
        "\n",
        "# Vectorizer yang telah di-fit dengan data training\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(x_train_70['stemmed_review'])\n",
        "\n",
        "# Model dengan parameter terbaik\n",
        "model = LinearSVC(C=1.0, max_iter=10000)\n",
        "clf = MultiOutputClassifier(model)\n",
        "\n",
        "# Model training\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "# Simpan model ke file pickle\n",
        "with open('model_svc_mlros_70.pkl', 'wb') as model_file:\n",
        "    pickle.dump(clf, model_file)\n",
        "\n",
        "with open('tfidf_vectorizer.pkl', 'wb') as vec_file:\n",
        "    pickle.dump(tfidf_vectorizer, vec_file)\n",
        "\n",
        "print(\"Model dan vectorizer berhasil disimpan!\")\n"
      ],
      "metadata": {
        "id": "ZZAh3Yf5ZPgJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}